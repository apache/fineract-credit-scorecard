{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Z5EFqHxB-zEn"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "test_size = 0.20\n",
        "\n",
        "processed_data = None\n",
        "categorical = None\n",
        "label_encoders = {}\n",
        "\n",
        "def preprocessing(dataset, data, test_size):\n",
        "    \"\"\"\n",
        "    Preprocess dataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data: DataFrame\n",
        "        Pandas dataframe containing German dataset.\n",
        "    \"\"\"\n",
        "    \n",
        "    global processed_data\n",
        "    global categorical\n",
        "    global label_encoders\n",
        "\n",
        "    # Reset global variables\n",
        "    \n",
        "    processed_data = None\n",
        "    categorical = None\n",
        "    label_encoders = {}\n",
        "\n",
        "\n",
        "    if dataset == \"German\":\n",
        "        # Drop savings account and checkings account columns as they contain a lot\n",
        "        # of NaN values and may not always be available in real life scenarios\n",
        "        data = data.drop(columns = ['Saving accounts', 'Checking account'])\n",
        "        \n",
        "    dat_dict = data.to_dict()\n",
        "    new_dat_dict = {}\n",
        "\n",
        "    # rename columns(Make them lowercase and snakecase)\n",
        "    for key, value in dat_dict.items():\n",
        "        newKey = key\n",
        "        if type(key) == str:\n",
        "            newKey = newKey.lower().replace(' ', '_')\n",
        "        # if newKey != key:\n",
        "        new_dat_dict[newKey] = dat_dict[key]\n",
        "    del dat_dict\n",
        "\n",
        "    data = pd.DataFrame.from_dict(new_dat_dict)\n",
        "    del new_dat_dict\n",
        "\n",
        "    # print(data.describe())\n",
        "    # print(data.describe(include='O'))\n",
        "    \n",
        "    if dataset == \"German\":\n",
        "        one_hot_columns = ['sex', 'housing', 'purpose']\n",
        "    else:\n",
        "        one_hot_columns = []\n",
        "    \n",
        "    # Drop null rows\n",
        "    data = data.dropna()\n",
        "\n",
        "    # if the column is not One-hot encoded, we will use categorical labelling\n",
        "    for column in data.columns:\n",
        "        if column not in one_hot_columns and data[column].dtype == 'object':\n",
        "            le = LabelEncoder()\n",
        "            data[column] = le.fit_transform(data[column])\n",
        "    if one_hot_columns:\n",
        "        data = pd.get_dummies(data, columns=one_hot_columns)\n",
        "    \n",
        "    # We dont need to normalise here as we are normalising after spliting into training and testing sets\n",
        "    \n",
        "    # for col in data.columns:\n",
        "    #     if(col not in categorical):\n",
        "    #         data[col] = (data[col].astype('float') - np.mean(data[col].astype('float')))/np.std(data[col].astype('float'))\n",
        "\n",
        "    # print(data.describe())\n",
        "    # print(data.describe(include='O'))\n",
        "\n",
        "    processed_data = data\n",
        "\n",
        "    # Get Training parameters\n",
        "    if dataset == \"German\":\n",
        "        target_col = data.columns[-1]\n",
        "        X = data.drop(columns=target_col, axis=1)\n",
        "        y = data[target_col].astype('int')\n",
        "    elif dataset == \"Australian\":\n",
        "        X = data.drop(columns=14, axis=1)\n",
        "        y = data[14].astype('int')\n",
        "    elif dataset == \"Japanese\":\n",
        "        X = data.drop(columns=15, axis=1)\n",
        "        y = data[15].astype('int')\n",
        "    elif dataset == \"Taiwan\":\n",
        "        X = data.drop(columns='default_payment_next_month', axis=1)\n",
        "        y = data['default_payment_next_month'].astype('int')\n",
        "    elif dataset == \"Polish\":\n",
        "        X = data.drop(columns='class', axis=1)\n",
        "        y = data['class'].astype('int')\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
        "    x_train = pd.DataFrame(x_train)\n",
        "    y_train = pd.DataFrame(y_train)\n",
        "\n",
        "    sc = StandardScaler()\n",
        "    x_train = sc.fit_transform(x_train)\n",
        "    x_test = sc.transform(x_test)\n",
        "\n",
        "    return (x_train, x_test, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
        "\n",
        "class Model(object):\n",
        "    \"\"\"\n",
        "    Basic Scorecard Model\n",
        "\n",
        "    Warning: This class should not be used directly. Use derived classes\n",
        "    instead.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 classifier=None,\n",
        "                 test_size=test_size,\n",
        "                 n_splits=1,\n",
        "                 random_state=None,\n",
        "                 n_jobs=None,\n",
        "                 params=None):\n",
        "                 \n",
        "        self.classifier = classifier\n",
        "        self.params = params\n",
        "        self.random_state = random_state\n",
        "        self.test_size = test_size\n",
        "        self.n_splits = n_splits\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "        self.model = GridSearchCV(estimator=classifier,\n",
        "                                  param_grid=params,\n",
        "                                  n_jobs=n_jobs,\n",
        "                                  cv=ShuffleSplit(test_size=test_size,\n",
        "                                  n_splits=n_splits,\n",
        "                                  random_state=0))\n",
        "    \n",
        "    def __str__(self):\n",
        "        return f\"\"\"\n",
        "        Model Object\n",
        "        ----------------------------------------------------------------\n",
        "\n",
        "        Classifier: {self.classifier.__class__.__name__}\n",
        "        Test Size: {self.test_size}\n",
        "        Random State: {self.random_state}\n",
        "        Number of Splits: {self.n_splits}\n",
        "        Parameter Grid: {self.params}\n",
        "\n",
        "        {self.model}\n",
        "        \"\"\"\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "\n",
        "    def train(self, x_train, y_train):\n",
        "        \"\"\"\n",
        "        Train scorecard model\n",
        "        \n",
        "        Args:\n",
        "            x_train:\n",
        "                array of training parameters\n",
        "            y_train:\n",
        "                pandas dataframe with training labels\n",
        "        \"\"\"\n",
        "\n",
        "        self.model = self.model.fit(x_train, y_train.values.ravel())\n",
        "        return self\n",
        "\n",
        "    def predict(self, data):\n",
        "        \"\"\"\n",
        "        Predict scorecard model\n",
        "\n",
        "        Args:\n",
        "            data: array\n",
        "                Data to perform prediction on.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.model.predict(data)\n",
        "\n",
        "    def accuracy(self, x_test, y_test):\n",
        "        \"\"\"\n",
        "        Compute scorecard model accuracy\n",
        "\n",
        "        Args:\n",
        "            x_test: array\n",
        "                The test parameters.\n",
        "            y_test: array\n",
        "                The labels\n",
        "        \"\"\"\n",
        "\n",
        "        y_pred = self.predict(x_test)\n",
        "        return accuracy_score(y_test, y_pred, normalize=False)\n",
        "\n",
        "    def metrics(self, x_test, y_test):\n",
        "        \"\"\"\n",
        "        Comput scorecard model metrics\n",
        "        \n",
        "        Args:\n",
        "            x_test: array\n",
        "                The test parameters.\n",
        "            y_test: array\n",
        "                The labels\n",
        "        \"\"\"\n",
        "\n",
        "        y_pred = self.predict(x_test)\n",
        "        \n",
        "        cm = confusion_matrix(y_pred, y_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred, normalize=True)\n",
        "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "        recall = recall_score(y_test, y_pred, average=\"macro\")\n",
        "        precision = precision_score(y_test, y_pred, average=\"macro\")\n",
        "\n",
        "        return {\"accuracy\" : accuracy,\n",
        "                \"f1_score\" : f1,\n",
        "                \"recall_score\" : recall,\n",
        "                \"precision_score\": precision}\n",
        "\n",
        "class RandomForest(Model):\n",
        "    def __init__(self,\n",
        "                 classifier=RandomForestClassifier(),\n",
        "                 test_size=test_size,\n",
        "                 n_splits=1,\n",
        "                 random_state=0,\n",
        "                 n_jobs=None,\n",
        "                 params={'n_estimators' : [20, 30, 40], 'random_state' : [0]}):        \n",
        "        super(RandomForest, self).__init__(classifier,\n",
        "                                           test_size,\n",
        "                                           n_splits,\n",
        "                                           random_state,\n",
        "                                           n_jobs,\n",
        "                                           params)\n",
        "\n",
        "class SVC(Model):\n",
        "    def __init__(self,\n",
        "                 classifier=SVC(),\n",
        "                 test_size=test_size,\n",
        "                 n_splits=1,\n",
        "                 random_state=0,\n",
        "                 n_jobs=None,\n",
        "                 params={'kernel' : ['poly'], 'degree' : [2, 3, 4]}):\n",
        "        super(SVC, self).__init__(classifier,\n",
        "                                  test_size,\n",
        "                                  n_splits,\n",
        "                                  random_state,\n",
        "                                  n_jobs,\n",
        "                                  params)\n",
        "\n",
        "class MLP(Model):\n",
        "    def __init__(self,\n",
        "                 classifier=MLPClassifier(),\n",
        "                 test_size=test_size,\n",
        "                 n_splits=1,\n",
        "                 random_state=0,\n",
        "                 n_jobs=-1,\n",
        "                 params={'hidden_layer_sizes' : [(100, 50 ,10)],\n",
        "                         'max_iter' : [500],\n",
        "                         'activation' : ['relu'],\n",
        "                         'solver' : ['adam'],\n",
        "                         'random_state' : [1]}):\n",
        "        super(MLP, self).__init__(classifier,\n",
        "                                  test_size,\n",
        "                                  n_splits,\n",
        "                                  random_state,\n",
        "                                  n_jobs,\n",
        "                                  params)\n",
        "\n",
        "class GradientBoost(Model):\n",
        "    def __init__(self,\n",
        "                 classifier=GradientBoostingClassifier(),\n",
        "                 test_size=test_size,\n",
        "                 n_splits=1,\n",
        "                 random_state=0,\n",
        "                 n_jobs=None,\n",
        "                 params={'n_estimators' : [100, 200, 50],\n",
        "                         'random_state' : [0],\n",
        "                         'learning_rate' : [1.0],\n",
        "                         'max_depth' : [1, 2, 3]}):\n",
        "        super(GradientBoost, self).__init__(classifier,\n",
        "                                            test_size,\n",
        "                                            n_splits,\n",
        "                                            random_state,\n",
        "                                            n_jobs,\n",
        "                                            params)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "{}\n",
            "\n",
            "RF\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest: {'accuracy': 0.995, 'f1_score': 0.49874686716791977, 'recall_score': 0.5, 'precision_score': 0.4975}\n",
            "\n",
            "SVC\n",
            "SVM: {'accuracy': 0.995, 'f1_score': 0.49874686716791977, 'recall_score': 0.5, 'precision_score': 0.4975}\n",
            "\n",
            "MLP\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP: {'accuracy': 0.995, 'f1_score': 0.49874686716791977, 'recall_score': 0.5, 'precision_score': 0.4975}\n",
            "\n",
            "GB\n",
            "Gradient Boost: {'accuracy': 0.975, 'f1_score': 0.6364958197019266, 'recall_score': 0.9874371859296482, 'precision_score': 0.5833333333333334}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['../zoo/models/german/gb_classifier.joblib']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# GERMAN DATASET\n",
        "german = pd.read_csv('../zoo/data/german.csv', index_col=0)\n",
        "x_train, x_test, y_train, y_test = preprocessing(\"German\", german, test_size)\n",
        "\n",
        "# Print Encoders\n",
        "print(categorical)\n",
        "print(label_encoders)\n",
        "\n",
        "# Set and Train the models\n",
        "print('\\nRF')\n",
        "RFmodel = RandomForest().train(x_train, y_train)\n",
        "print(f\"Random Forest: {RFmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nSVC\")\n",
        "SVCmodel = SVC().train(x_train, y_train)\n",
        "print(f\"SVM: {SVCmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nMLP\")\n",
        "MLPmodel = MLP().train(x_train, y_train)\n",
        "print(f\"MLP: {MLPmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nGB\")\n",
        "GBmodel = GradientBoost().train(x_train, y_train)\n",
        "print(f\"Gradient Boost: {GBmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "# Save Training Data\n",
        "joblib.dump(categorical, \"../zoo/models/german/categorical.joblib\", compress=True)\n",
        "joblib.dump(label_encoders, \"../zoo/models/german/label_encoders.joblib\", compress=True)\n",
        "\n",
        "joblib.dump(RFmodel.model, \"../zoo/models/german/rf_classifier.joblib\", compress=True)\n",
        "joblib.dump(SVCmodel.model, \"../zoo/models/german/svc_classifier.joblib\", compress=True)\n",
        "joblib.dump(MLPmodel.model, \"../zoo/models/german/mlp_classifier.joblib\", compress=True)\n",
        "joblib.dump(GBmodel.model, \"../zoo/models/german/gb_classifier.joblib\", compress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "{}\n",
            "\n",
            "RF\n",
            "Random Forest: {'accuracy': 0.8840579710144928, 'f1_score': 0.8828522920203734, 'recall_score': 0.8841813923781137, 'precision_score': 0.8819047619047619}\n",
            "\n",
            "SVC\n",
            "SVM: {'accuracy': 0.8478260869565217, 'f1_score': 0.8454812050119969, 'recall_score': 0.8449010006387055, 'precision_score': 0.8461538461538461}\n",
            "\n",
            "MLP\n",
            "MLP: {'accuracy': 0.8768115942028986, 'f1_score': 0.8753387533875339, 'recall_score': 0.8759846710666382, 'precision_score': 0.8747877758913413}\n",
            "\n",
            "GB\n",
            "Gradient Boost: {'accuracy': 0.8695652173913043, 'f1_score': 0.8667667882428662, 'recall_score': 0.8643815201192251, 'precision_score': 0.8706952566601689}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['../zoo/models/australian/gb_classifier.joblib']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Australian DATASET\n",
        "australian = [i.strip().split() for i in open(\"../zoo/data/australian.dat\").readlines()]\n",
        "australian = pd.DataFrame(australian)\n",
        "x_train, x_test, y_train, y_test = preprocessing(\"Australian\", australian, test_size)\n",
        "\n",
        "# Print Encoders\n",
        "print(categorical)\n",
        "print(label_encoders)\n",
        "\n",
        "# Set and Train the models\n",
        "print('\\nRF')\n",
        "RFmodel = RandomForest().train(x_train, y_train)\n",
        "print(f\"Random Forest: {RFmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nSVC\")\n",
        "SVCmodel = SVC().train(x_train, y_train)\n",
        "print(f\"SVM: {SVCmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nMLP\")\n",
        "MLPmodel = MLP().train(x_train, y_train)\n",
        "print(f\"MLP: {MLPmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nGB\")\n",
        "GBmodel = GradientBoost().train(x_train, y_train)\n",
        "print(f\"Gradient Boost: {GBmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "# Save Training Data\n",
        "joblib.dump(categorical, \"../zoo/models/australian/categorical.joblib\", compress=True)\n",
        "joblib.dump(label_encoders, \"../zoo/models/australian/label_encoders.joblib\", compress=True)\n",
        "\n",
        "joblib.dump(RFmodel.model, \"../zoo/models/australian/rf_classifier.joblib\", compress=True)\n",
        "joblib.dump(SVCmodel.model, \"../zoo/models/australian/svc_classifier.joblib\", compress=True)\n",
        "joblib.dump(MLPmodel.model, \"../zoo/models/australian/mlp_classifier.joblib\", compress=True)\n",
        "joblib.dump(GBmodel.model, \"../zoo/models/australian/gb_classifier.joblib\", compress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "{}\n",
            "\n",
            "RF\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest: {'accuracy': 0.8478260869565217, 'f1_score': 0.8460066953610712, 'recall_score': 0.8480769230769231, 'precision_score': 0.8447619047619048}\n",
            "\n",
            "SVC\n",
            "SVM: {'accuracy': 0.8333333333333334, 'f1_score': 0.831841059602649, 'recall_score': 0.8352564102564102, 'precision_score': 0.8306638566912539}\n",
            "\n",
            "MLP\n",
            "MLP: {'accuracy': 0.8043478260869565, 'f1_score': 0.799709724238026, 'recall_score': 0.7980769230769231, 'precision_score': 0.8021442495126705}\n",
            "\n",
            "GB\n",
            "Gradient Boost: {'accuracy': 0.8478260869565217, 'f1_score': 0.8464635761589403, 'recall_score': 0.8500000000000001, 'precision_score': 0.8452054794520548}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['../zoo/models/japanese/gb_classifier.joblib']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Japanese DATASET\n",
        "japanese = [i.strip().split(\",\") for i in open(\"../zoo/data/japanese/japanese.data\").readlines()]\n",
        "japanese = pd.DataFrame(japanese)\n",
        "x_train, x_test, y_train, y_test = preprocessing(\"Japanese\", japanese, test_size)\n",
        "\n",
        "# Print Encoders\n",
        "print(categorical)\n",
        "print(label_encoders)\n",
        "\n",
        "# Set and Train the models\n",
        "print('\\nRF')\n",
        "RFmodel = RandomForest().train(x_train, y_train)\n",
        "print(f\"Random Forest: {RFmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nSVC\")\n",
        "SVCmodel = SVC().train(x_train, y_train)\n",
        "print(f\"SVM: {SVCmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nMLP\")\n",
        "MLPmodel = MLP().train(x_train, y_train)\n",
        "print(f\"MLP: {MLPmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nGB\")\n",
        "GBmodel = GradientBoost().train(x_train, y_train)\n",
        "print(f\"Gradient Boost: {GBmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "# Save Training Data\n",
        "joblib.dump(categorical, \"../zoo/models/japanese/categorical.joblib\", compress=True)\n",
        "joblib.dump(label_encoders, \"../zoo/models/japanese/label_encoders.joblib\", compress=True)\n",
        "\n",
        "joblib.dump(RFmodel.model, \"../zoo/models/japanese/rf_classifier.joblib\", compress=True)\n",
        "joblib.dump(SVCmodel.model, \"../zoo/models/japanese/svc_classifier.joblib\", compress=True)\n",
        "joblib.dump(MLPmodel.model, \"../zoo/models/japanese/mlp_classifier.joblib\", compress=True)\n",
        "joblib.dump(GBmodel.model, \"../zoo/models/japanese/gb_classifier.joblib\", compress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "{}\n",
            "\n",
            "RF\n",
            "Random Forest: {'accuracy': 0.8206666666666667, 'f1_score': 0.6795023907255062, 'recall_score': 0.6548772504091653, 'precision_score': 0.7495203661143801}\n",
            "\n",
            "SVC\n",
            "SVM: {'accuracy': 0.8055, 'f1_score': 0.6079709000415936, 'recall_score': 0.5940016366612111, 'precision_score': 0.7342833157727924}\n",
            "\n",
            "MLP\n",
            "MLP: {'accuracy': 0.7665, 'f1_score': 0.6386418349497656, 'recall_score': 0.6325450081833061, 'precision_score': 0.6470625889008195}\n",
            "\n",
            "GB\n",
            "Gradient Boost: {'accuracy': 0.8195, 'f1_score': 0.6696545694047971, 'recall_score': 0.6449509001636661, 'precision_score': 0.7516252007617654}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['../zoo/models/taiwan/gb_classifier.joblib']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Taiwan DATASET\n",
        "taiwan = pd.read_excel('../zoo/data/taiwan.xls', index_col=0, header=1)\n",
        "x_train, x_test, y_train, y_test = preprocessing(\"Taiwan\", taiwan, test_size)\n",
        "\n",
        "# Print Encoders\n",
        "print(categorical)\n",
        "print(label_encoders)\n",
        "\n",
        "# Set and Train the models\n",
        "print('\\nRF')\n",
        "RFmodel = RandomForest().train(x_train, y_train)\n",
        "print(f\"Random Forest: {RFmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nSVC\")\n",
        "SVCmodel = SVC().train(x_train, y_train)\n",
        "print(f\"SVM: {SVCmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nMLP\")\n",
        "MLPmodel = MLP().train(x_train, y_train)\n",
        "print(f\"MLP: {MLPmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nGB\")\n",
        "GBmodel = GradientBoost().train(x_train, y_train)\n",
        "print(f\"Gradient Boost: {GBmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "# Save Training Data\n",
        "joblib.dump(categorical, \"../zoo/models/taiwan/categorical.joblib\", compress=True)\n",
        "joblib.dump(label_encoders, \"../zoo/models/taiwan/label_encoders.joblib\", compress=True)\n",
        "\n",
        "joblib.dump(RFmodel.model, \"../zoo/models/taiwan/rf_classifier.joblib\", compress=True)\n",
        "joblib.dump(SVCmodel.model, \"../zoo/models/taiwan/svc_classifier.joblib\", compress=True)\n",
        "joblib.dump(MLPmodel.model, \"../zoo/models/taiwan/mlp_classifier.joblib\", compress=True)\n",
        "joblib.dump(GBmodel.model, \"../zoo/models/taiwan/gb_classifier.joblib\", compress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "{}\n",
            "\n",
            "RF\n",
            "Random Forest: {'accuracy': 0.9782173259889835, 'f1_score': 0.5550920537610866, 'recall_score': 0.5345633905736208, 'precision_score': 0.6901985423473235}\n",
            "\n",
            "SVC\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM: {'accuracy': 0.9789684526790186, 'f1_score': 0.49468623481781376, 'recall_score': 0.5, 'precision_score': 0.4894842263395093}\n",
            "\n",
            "MLP\n",
            "MLP: {'accuracy': 0.9802203304957436, 'f1_score': 0.7148243851605213, 'recall_score': 0.6811990013396663, 'precision_score': 0.7651988110993766}\n",
            "\n",
            "GB\n",
            "Gradient Boost: {'accuracy': 0.9822233350025038, 'f1_score': 0.6574014002404209, 'recall_score': 0.6006789672390696, 'precision_score': 0.8963299893327581}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['../zoo/models/polish/gb_classifier.joblib']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Polish DATASET\n",
        "from scipy.io import arff\n",
        "\n",
        "year_1 = pd.DataFrame(arff.loadarff('../zoo/data/polish/1year.arff')[0])\n",
        "year_2 = pd.DataFrame(arff.loadarff('../zoo/data/polish/2year.arff')[0])\n",
        "year_3 = pd.DataFrame(arff.loadarff('../zoo/data/polish/3year.arff')[0])\n",
        "year_4 = pd.DataFrame(arff.loadarff('../zoo/data/polish/4year.arff')[0])\n",
        "year_5 = pd.DataFrame(arff.loadarff('../zoo/data/polish/5year.arff')[0])\n",
        "polish = pd.concat([year_1, year_2, year_3, year_4, year_5], ignore_index=True)\n",
        "x_train, x_test, y_train, y_test = preprocessing(\"Polish\", polish, test_size)\n",
        "\n",
        "# Print Encoders\n",
        "print(categorical)\n",
        "print(label_encoders)\n",
        "\n",
        "# Set and Train the models\n",
        "print('\\nRF')\n",
        "RFmodel = RandomForest().train(x_train, y_train)\n",
        "print(f\"Random Forest: {RFmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nSVC\")\n",
        "SVCmodel = SVC().train(x_train, y_train)\n",
        "print(f\"SVM: {SVCmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nMLP\")\n",
        "MLPmodel = MLP().train(x_train, y_train)\n",
        "print(f\"MLP: {MLPmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "print(\"\\nGB\")\n",
        "GBmodel = GradientBoost().train(x_train, y_train)\n",
        "print(f\"Gradient Boost: {GBmodel.metrics(x_test, y_test)}\")\n",
        "\n",
        "# Save Training Data\n",
        "joblib.dump(categorical, \"../zoo/models/polish/categorical.joblib\", compress=True)\n",
        "joblib.dump(label_encoders, \"../zoo/models/polish/label_encoders.joblib\", compress=True)\n",
        "\n",
        "joblib.dump(RFmodel.model, \"../zoo/models/polish/rf_classifier.joblib\", compress=True)\n",
        "joblib.dump(SVCmodel.model, \"../zoo/models/polish/svc_classifier.joblib\", compress=True)\n",
        "joblib.dump(MLPmodel.model, \"../zoo/models/polish/mlp_classifier.joblib\", compress=True)\n",
        "joblib.dump(GBmodel.model, \"../zoo/models/polish/gb_classifier.joblib\", compress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "metadata": {
      "interpreter": {
        "hash": "434fba307fd1171c9cfc17821a2afcf8929f30379beeaab9e3fdf8c6db2d1c93"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
